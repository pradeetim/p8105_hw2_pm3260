---
title: "p8105_hw2_pm3260"
author: "Pradeeti Mainali"
date: "2024-10-02"
output: github_document
---

```{r setup, echo=FALSE, message=FALSE}
library(tidyverse)
library(readxl)
library(tidyr)
library(knitr)
```

# Problem 1

### Importing transit data:

```{r nyctransit, results='hide'}
nyc_df = 
  read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") |>
  janitor::clean_names() |>
  select(
    line, station_name, station_latitude, 
    station_longitude, starts_with("route"), 
    entrance_type, entry, vending, ada
    ) |>
  mutate(
    across(
      c(route8, route9, route10, route11), as.character),
    entry_logical = ifelse(entry == "YES", TRUE, FALSE))
```

The NYC Transit data frame contains the variables that describe routes, entry and entrance type, ADA compliance, station name/latitude/longitude, lines, and vending. 

So far, I have cleaned the names to make them all lower case and selected the variables above to remain in the data set.

The dimensions of this data set is `r nrow(nyc_df)` rows and `r ncol(nyc_df)` columns. The data is NOT tidy. 

### Finding distinct stations:

```{r stations, results='hide'}
distinct_stations = 
  nyc_df |>
  distinct(station_name, line) |>
  nrow()

distinct_stations
```

There are 465 distinct stations. 

### Finding number of ADA compliant stations:

```{r ada, results='hide'}
ada_compliant = 
  nyc_df |>
  filter(ada == TRUE) |>
  distinct(station_name, line) |>
  nrow()

ada_compliant
```

There are 84 ADA compliant stations.

### Proportion of vending 

```{r vending, results='hide'}
no_vending = 
  nyc_df |>
  filter(vending == "NO") |>
  summarise(
    proportion = mean(entry_logical, na.rm = TRUE)) |>
  pull(proportion)

no_vending
```

37% of station entrances/exits without vending allow entrance.

### Reformatting data:

```{r reformat, results='hide'}
nyc_df |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A") |> 
  select(station_name, line) |> 
  distinct()
```

There are 60 distinct stations that serve the A train. 

```{r pivot, results='hide'}
nyc_df |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A", ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```

Of the station that serve the A train, 17 are the ADA compliant.

# Problem 2

### Importing Mr. Trash Wheel Data

```{r mrtrash, results='hide'}
mrtrash_df = 
  read_excel("./data/202409 Trash Wheel Collection Data.xlsx", 
    sheet = "Mr. Trash Wheel") |>
  janitor::clean_names() |>
  drop_na(dumpster) |>
  select(where(~ !all(is.na(.)))) |>
  mutate(
    sports_balls = as.integer(
      round(sports_balls, 0)),
    year = as.numeric(year),
    trash_wheel = "Mr. Trash Wheel")
```

### Importing Prof. Trash Wheel Data

```{r proftrash}
proftrash_df = 
  read_excel("./data/202409 Trash Wheel Collection Data.xlsx", 
    sheet = "Professor Trash Wheel") |>
  janitor::clean_names() |>
  drop_na(dumpster) |>
  mutate(trash_wheel = "Professor Trash Wheel")
```

### Importing Gwynnda Trash Wheel Data

```{r gwyntrash}
gwyntrash_df = 
  read_excel("./data/202409 Trash Wheel Collection Data.xlsx", 
    sheet = "Gwynnda Trash Wheel") |>
  janitor::clean_names() |>
  drop_na(dumpster) |>
  mutate(trash_wheel = "Gwynnda Trash Wheel")

```

### Merging datasets 

```{r}
trash_df =
  bind_rows(mrtrash_df, proftrash_df, gwyntrash_df)
```

### Paragraph description of data:

The three data sets are from different sheets of the same excel file. They were imported so that the rows and columns all had data and then merged. Before the merge, a new variable to indicate which sheet they came from was added. 

After the merge in `trash_df`, there were `r nrow(trash_df)` observations and `r ncol(trash_df)` variables (`r paste(names(trash_df), collapse = ", ")`). 

The total weight of trash collected by Professor Trash Wheel was `r sum(pull(filter(trash_df, trash_wheel == "Professor Trash Wheel"), weight_tons), na.rm = TRUE)` tons. 

The total number of cigarette butts collected by Gwynnda in June of 2022 was `r format(sum(pull(filter(trash_df, trash_wheel == "Gwynnda Trash Wheel", month == "June", year == "2022"), cigarette_butts), na.rm = TRUE), scientific = FALSE)`.

# Problem 3

### Importing bakers dataset

```{r bakers}
bakers_df = 
  read.csv("data/gbb_datasets/bakers.csv") |>
  janitor::clean_names() |>
  mutate(
    baker = word(baker_name, 1),
    baker = if_else(
      baker == "Jo", "Joanne", baker)) |>
  select(baker, series, baker_age, baker_occupation, hometown) |>
  arrange(series)
```

### Importing bakes dataset

```{r bakes}
bakes_df = 
  read.csv("data/gbb_datasets/bakes.csv") |>
  janitor::clean_names() |>
  select(baker, series, episode, everything()) |>
  mutate(baker = if_else(
    baker == '"Jo"', "Joanne", baker))
```

### Importing results dataset

```{r results}
results_df = 
  read.csv("data/gbb_datasets/results.csv", skip=2) |>
  janitor::clean_names() |>
  select(baker, series, episode, everything())

```

### Merging with left_join

```{r merging}
bakes_results_df = 
 left_join(
   results_df, bakes_df, by = c("baker", "series", "episode")
   )

bakeoff_df=
  left_join(
    bakes_results_df, bakers_df, by = c("baker", "series")) |>
  select(
    baker, series, episode, everything()
    )
```

### Export

```{r}
write_csv(bakeoff_df, "data/gbb_datasets/bakeoff.csv")
```

### Data cleaning process:

I first started by importing all three data sets and taking a look at the variables that were present in each one. When I saw that some variables were repeating like name of the baker and series, it gave me a point to start off from. 

starting with the bakers data set, there were several things I had to change. First of all, the names included both first and last name so I had to keep just the first to match the other two data sets. Second, I realized the name 'Jo' was typed three different ways through out the data sets. I decided to keep it long as 'Joanne'  and changed the bakers and bakes data set to match. 

When merging, I decided to use two steps because I realized baker data set did not have the episodes variable. So I first merged results and bakes (by baker, series and episode). Then I did that data set + baker_df by baker and series. I think this helped keep the integrity of the data. 

the final data set has the name of the baker, their series, and the episode in that order. Then it gives details of the bakers (from baker data set), their bakes, and is they won or not in that episode.

### Creating a table

```{r}
table_df = bakeoff_df |>
  filter(
    result %in% c("STAR BAKER", "WINNER"), 
    series >= 5) |>
  select(series, episode, baker, result) |>
  arrange(series, episode) |>
  knitr::kable()

table_df
```

### Comments:

Looking at the table, we can see that most of the winners had gotten star baker earlier in the season at  least once. The number of times you got star bakers doesn't seem to matter as winners ranged from being the star baker from as low as 1 time to as high as 3 times per their season. However, in Season 10, the winner was David, who has not won star baker even once during the season, which seems like a surprise. 


### Importing the viewership data:

```{r viewers}
viewers_df = 
  read.csv("data/gbb_datasets/viewers.csv") |>
  janitor::clean_names() |>
  pivot_longer(
    cols = series_1:series_10,
    names_to = "series",
    values_to = "viewership",
    names_prefix = "series_") |>
  mutate(series = as.numeric(series)) |>
  select(series, episode, viewership) |>
  arrange(series, episode) |>
  drop_na(viewership) 

viewers_tbl = viewers_df |>
  head(10) |>
  knitr::kable()

viewers_tbl
```

The average viewership in Season 1 was `r mean(pull(filter(viewers_df, series == 1), viewership))` million, and the average viewership in Season 5 was `r mean(pull(filter(viewers_df, series == 5), viewership))` million.

